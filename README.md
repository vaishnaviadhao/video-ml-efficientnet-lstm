# AI vs Real Video Classification using EfficientNet + LSTM

This project implements an end-to-end **video classification pipeline** to distinguish **AI-generated (Fake)** videos from **Real** videos using deep learning.

The system follows a **frame-based + temporal modeling approach**, combining a CNN for spatial feature extraction and an LSTM for temporal understanding.

---

## ğŸ” Problem Statement

Given a video, determine whether it is:
- **AI-generated (Fake)**
- **Authentic (Real)**

Key challenges:
- Frame-level noise
- Temporal inconsistencies in AI-generated videos
- Limited labeled video data

---

## Architecture Overview
Video (.mp4)
â†’ Frame Extraction (1 FPS)
â†’ EfficientNet-B3 (frame-level features)
â†’ Temporal Sequence Builder (fixed length)
â†’ LSTM-based Classifier
â†’ Fake / Real Prediction


---

## ğŸ§© Pipeline Breakdown

### 1ï¸âƒ£ Frame Extraction
- Videos are converted to frames at **1 frame per second**
- Stored as:
Frames/
â”œâ”€â”€ Fake/<video_id>/frame_xxxx.jpg
â””â”€â”€ Real/<video_id>/frame_xxxx.jpg


### 2ï¸âƒ£ Feature Extraction (EfficientNet-B3)
- Pretrained **EfficientNet-B3** (ImageNet weights)
- Classification head removed
- Global Average Pooling used
- Output: **1536-dim feature per frame**

### 3ï¸âƒ£ Temporal Sequence Construction
- Each video converted into a fixed-length sequence of **64 frames**
- Padding or uniform sampling applied

### 4ï¸âƒ£ Temporal Modeling
- **LSTM-based neural network**
- Learns temporal patterns across frames
- Binary classification (Fake vs Real)

---

## ğŸ“Š Results

| Metric | Value |
|------|------|
| Validation Accuracy | **100%** |
| Validation ROC-AUC | **1.00** |
| Precision / Recall / F1 | **1.00 / 1.00 / 1.00** |

> âš ï¸ Note: Dataset size is small. High accuracy indicates proof-of-concept rather than generalization.

---

## ğŸ“ Project Structure

Video_ML_Code/
â”œâ”€â”€ Dataset/ # Raw videos (Fake / Real)
â”œâ”€â”€ Frames/ # Extracted frames
â”œâ”€â”€ Metadata/ # Frame metadata CSVs
â”œâ”€â”€ Features_V1/ # EfficientNet features (.npy)
â”œâ”€â”€ Sequences_V1/ # Fixed-length sequences
â”œâ”€â”€ extract_frames.py
â”œâ”€â”€ feature_extraction_b3_V1.py
â”œâ”€â”€ build_sequences_b3_V1.py
â”œâ”€â”€ temporal_lstm_v1.py
â”œâ”€â”€ train_temporal_model_v1.py
â”œâ”€â”€ video_labels.csv
â”œâ”€â”€ best_temporal_model.keras
â””â”€â”€ README.md


---

## ğŸ› ï¸ Tech Stack

- Python 3.12
- TensorFlow / Keras
- EfficientNet-B3
- NumPy, Pandas
- OpenCV
- scikit-learn

---

## âš™ï¸ Environment Notes

- TensorFlow requires **Python â‰¤ 3.12**
- NumPy pinned to `<2.0` for TensorFlow compatibility
- Tested on **Windows & macOS**

---

## ğŸš€ How to Run

```bash
# Extract frames
python extract_frames.py

# Generate metadata
python create_metadata_csv.py

# Extract CNN features
python feature_extraction_b3_V1.py

# Build temporal sequences
python build_sequences_b3_V1.py

# Train temporal model
python train_temporal_model_v1.py

âš ï¸ Trained model file (`best_temporal_model.keras`) is not committed to GitHub due to size constraints.
It can be regenerated by running `train_temporal_model_v1.py`.

## ğŸ“Š Results

| Metric | Value |
|------|------|
| Validation Accuracy | 1.00 |
| Validation ROC-AUC | 1.00 |
| Precision | 1.00 |
| Recall | 1.00 |
| F1-score | 1.00 |

> Note: Dataset size is limited. Results demonstrate proof-of-concept.


